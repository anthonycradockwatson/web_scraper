***Web Scraper â€” Image + Class Collector***

This small scraper collects image URLs and DOM class names from a webpage and can (optionally) download images into training/validation folders. It uses Selenium to render pages (so it can handle JavaScript-injected content) and falls back to requests/BeautifulSoup for simple parsing where appropriate.

Important: This scraper does NOT work reliably on regular Google Search results pages. Google aggressively blocks automated scraping and often serves images through dynamic blobs, lazy-loading, or via CDNs that require a browser session. Use the scraper on pages you control or on sites that allow scraping.

Features
- Render pages with Selenium (Chrome) to capture JS-generated <img> elements and class attributes
- Heuristic to find the dominant image class in a page and collect image srcs
- Save images into `assets/images/training` and `assets/images/validation` with indexed filenames
- Optionally validate images with Pillow before saving
- Produces class-frequency counts (useful for debugging and building selectors)

Requirements
- Python 3.8+
- Packages (install with pip):
  - selenium
  - webdriver-manager (optional but convenient)
  - requests
  - beautifulsoup4
  - pillow
  - collections (standard lib)

Example setup (PowerShell):

```powershell
pip install -r requirements.txt
```

Quick usage
- Edit or run the `web_scraper.py` script. By default it runs against an example URL at the bottom of the file; change the URL to your target.
- The script will create these directories (if missing) and place 80% of images into training and 20% into validation:
  - `assets/images/training`
  - `assets/images/validation`

Notes about Google Search pages
- Google Search pages (images.google.com and the regular search results) are difficult to scrape because:
  - Many image links are provided as `blob:` URLs that are only available inside an active browser context.
  - Content is lazy-loaded and generated by JavaScript; while Selenium can render it, Google often blocks automated drivers or requires complex cookie/session handling.
  - Scraping Google may violate their terms of service; prefer site APIs or other image sources.

How the script works (high-level)
1. Launch Selenium Chrome and open the page.
2. Scroll to trigger lazy-loading.
3. Collect all `<img>` elements and their `class` attributes.
4. Choose the most common class (heuristic) and select images by that class.
5. Download images (requests) and save them into training/validation split (80/20 by default).
6. Use Pillow to optionally validate images.

Troubleshooting
- No files are written:
  - Check your current working directory when running the script. The script uses relative paths (assets/...). Run it from the project root or change paths to absolute.
  - Print `os.getcwd()` in the script to confirm where files will be written.
  - Some URLs may be `blob:` or `data:` URLs. `data:` can be decoded and saved, `blob:` cannot be fetched by requests; you need to extract the underlying binary in the browser (canvas toDataURL) or find the real CDN URL.
- Requests return non-image responses (403/503):
  - Try adding a browser-like User-Agent header to `requests.get`.
  - If the server refuses programmatic clients, consider using the browser session to fetch bytes or respect the site's policy.
- Images are corrupt:
  - Pillow can detect and optionally reject corrupted files; the script can be extended to validate before saving.

Ethics and legality
- Respect robots.txt and a site's terms of service.
- Do not use the scraper against sites that explicitly forbid automation.

Extending the tool
- Add command-line flags with `argparse` (URL, output folder, max images, download flag).
- Use `webdriver-manager` to avoid manual ChromeDriver installs.
- Add concurrency (ThreadPoolExecutor) for faster downloads.

License
MIT
