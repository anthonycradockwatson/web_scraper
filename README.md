**Web Scraper â€” Image + Class Collector**

This small scraper collects image URLs and DOM class names from a webpage and can (optionally) download images into training/validation folders. It uses Selenium to render pages (so it can handle JavaScript-injected content) and falls back to requests/BeautifulSoup for simple parsing where appropriate.

Important: This scraper does NOT work reliably on regular Google Search results pages. Google aggressively blocks automated scraping and often serves images through dynamic blobs, lazy-loading, or via CDNs that require a browser session. Use the scraper on pages you control or on sites that allow scraping.

Features
- Render pages with Selenium (Chrome) to capture JS-generated <img> elements and class attributes
- Heuristic to find the dominant image class in a page and collect image srcs
- Save images into `assets/images/training` and `assets/images/validation` with indexed filenames
- Optionally validate images with Pillow before saving
- Produces class-frequency counts (useful for debugging and building selectors)

Requirements
- Python 3.8+
- Packages (install with pip):
  - selenium
  - webdriver-manager (optional but convenient)
  - requests
  - collections (standard lib)

Example setup (PowerShell):

```powershell
pip -m venv .venv
pip install -r requirements.txt
```

Quick usage
- Edit or run the `web_scraper.py` script. By default it runs against an example URL at the bottom of the file; change the URL to your target.
- The script will create these directories (if missing) and place 80% of images into training and 20% into validation:
  - `assets/images/training`
  - `assets/images/validation`

Notes about Google Search pages
- Google Search pages (images.google.com and the regular search results) are difficult to scrape because:
  - Many image links are provided as `blob:` URLs that are only available inside an active browser context.
  - Content is lazy-loaded and generated by JavaScript; while Selenium can render it, Google often blocks automated drivers or requires complex cookie/session handling.
  - Scraping Google may violate their terms of service; prefer site APIs or other image sources.

How the script works (high-level)
1. Launch Selenium Chrome and open the page.
2. Scroll to trigger lazy-loading.
3. Collect all `<img>` elements and their `class` attributes.
4. Choose the most common class (heuristic) and select images by that class (to exclude things like icons and profile pictures).
5. Download images (requests) and save them into training/validation split (80/20 by default).

